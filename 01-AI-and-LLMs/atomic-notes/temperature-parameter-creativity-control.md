---
state: fleeting
type: research-atomic
created: 2025-06-17
source-credibility: 8
research-context: prompt-engineering-methodologies
validation-status: verified
---

# Temperature Parameter Creativity Control

## Core Concept
Temperature controls the randomness or "creativity" of LLM output by modifying how the model samples from its predicted probability distribution. Low temperature (closer to 0) produces deterministic, focused outputs, while high temperature (closer to 1) produces diverse, creative outputs.

## Research Context
Temperature adjustment is often the first modification made when initial prompting attempts yield unsatisfactory results. For iterative refinement processes, initial thought generation might benefit from higher temperature, while critique and final answers prefer lower temperature.

## Source Quality
- **Primary Source**: "Prompt Engineering: A Comprehensive Guide to Iterative Refinement for Large Language Models"
- **Secondary Source**: "Recursive Learning Prompt Engineering Best Practices"
- **Credibility Score**: 8/10
- **Validation Method**: Verified through Perplexity search and multiple technical documentation sources

## Connection Potential
- Links to other sampling parameters (top-k, top-p)
- Connects to iterative refinement workflows
- Relates to creative vs factual task optimization
- Bridges to self-consistency prompting techniques